head(housing_data_set1)
glimpse(housing_data)
housing_data_set1 <- housing_data[, `Sale Price`]
housing_data_set1 <- housing_data[, housing_data$`Sale Date`]
housing_data_set1 <- housing_data %>% select(zip5)
housing_data_set1 <- housing_data %>% dplyr::select(zip5)
head(housing_data_set1)
housing_data_set1 <- housing_data %>% dplyr::select(`Sale Price`,bedrooms,bath_full_count,sq_ft_lot)
head(housing_data_set1)
pairs(housing_data_set1)
ggplot(housing_data_set1, aes (x = `Sale Price`)) +
geom_histogram(binwidth = 10000) +
labs(x = "Sale Price")
## housing_data_set1 %>%
##  filter(housing_data_set1$`Sale Price` > 1000000) %>%
## summarize(n())
ggplot(data = housing_data_set1, aes(x = sq_ft_lot, y = `Sale Price`)) +
geom_point() +
labs( title = "Plotting Sale Price against Lot size",
x = "Lot size in sq ft",
y = "Sale Price")
var1 <- lm(`Sale Price` ~ sq_ft_lot, data = housing_data_set1)
var2 <- lm(`Sale Price` ~ bedrooms + bath_full_count, data = housing_data_set1)
summary(var1)
## predict(var1, data.frame(sq_ft_lot = 6635))
summary(var2)
## predict(var2, data.frame(bedrooms = 5, bath_full_count = 3))
library(QuantPsyc)
lm.beta(var2)
anova(var1, var2)
library(broom)
augment(var2)
df1$large.residual <- var2$.std.resid < 1
df1 <- var2$.std.resid < 1
head(df1)
var2$.std.resid < 1
var2$.std.resid
var2
augment(var2)$.std.resid
augment(var2)$.std.resid < -2
augment(var2)$large.residual <- augment(var2)$.std.resid > 2 | augment(var2)$.std.resid < -2
library(broom)
housing_data_set1.augment <- augment(var2)
head(housing_data_set1.augment)
housing_data_set1.augment$large.residual <- housing_data_set1.augment$.std.resid > 2 | housing_data_set1.augment$.std.resid < -2
head(housing_data_set1.augment)
sum(housing_data_set1.augment$large.residual)
housing_data.large_resid <- housing_data_set1.augment %>%
dplyr::filter(large.residual == TRUE)
housing_data.large_resid
housing_data.large_resid <- housing_data_set1.augment %>%
dplyr::select(`Sale Price`, bedrooms,bath_full_count,large.residual)
housing_data.large_resid <- housing_data_set1.augment %>%
dplyr::select(Sale.Price, bedrooms,bath_full_count,large.residual)
dplyr::filter(large.residual == TRUE)
housing_data.large_resid <- housing_data_set1.augment %>%
dplyr::select(Sale.Price, bedrooms,bath_full_count,large.residual) %>%
dplyr::filter(large.residual == TRUE)
housing_data.large_resid
housing_data.large_resid <- housing_data_set1.augment %>%
dplyr::select(Sale.Price, bedrooms,bath_full_count,.std.resid,large.residual) %>%
dplyr::filter(large.residual == TRUE)
housing_data.large_resid
knitr::opts_chunk$set(echo = FALSE)
cooks.distance(housing_data_set1)
head(housing_data_set1)
cooks.distance(var2)
housing_data_set1.augment$cooks.distance <- cooks.distance(var2)
housing_data_set1.augment$levarage <- hatvalues(var2)
housing_data_set1.augment$cov.ratios <- covratio(var2)
head(housing_data_set1.augment)
housing_data.lev <- housing_data_set1.augment %>%
dplyr::select(Sale.Price, bedrooms,bath_full_count,.hat,.cooksd,cov.ratios,large.residual) %>%
dplyr::filter(large.residual == TRUE)
housing_data.lev
housing_data.lev <- housing_data_set1.augment %>%
dplyr::select(Sale.Price, bedrooms,bath_full_count,.hat,.cooksd,cov.ratios,large.residual) %>%
dplyr::filter(large.residual == TRUE)
housing_data.lev %>%
arrange(asc(.cooksd))
housing_data.lev <- housing_data_set1.augment %>%
dplyr::select(Sale.Price, bedrooms,bath_full_count,.hat,.cooksd,cov.ratios,large.residual) %>%
dplyr::filter(large.residual == TRUE)
housing_data.lev %>%
arrange(.cooksd)
housing_data.lev <- housing_data_set1.augment %>%
dplyr::select(Sale.Price, bedrooms,bath_full_count,.hat,.cooksd,cov.ratios,large.residual) %>%
dplyr::filter(large.residual == TRUE)
housing_data.lev %>%
arrange(desc(.cooksd))
View(housing_data)
housing_data.lev <- housing_data_set1.augment %>%
dplyr::select(Sale.Price, bedrooms,bath_full_count,.hat,.cooksd,cov.ratios,large.residual) %>%
dplyr::filter(large.residual == TRUE)
housing_data.lev %>%
arrange(desc(.hat))
housing_data.lev <- housing_data_set1.augment %>%
dplyr::select(Sale.Price, bedrooms,bath_full_count,.hat,.cooksd,cov.ratios,large.residual) %>%
dplyr::filter(large.residual == TRUE)
housing_data.lev %>%
arrange(desc(cov.ratios))
housing_data.lev <- housing_data_set1.augment %>%
dplyr::select(Sale.Price, bedrooms,bath_full_count,.hat,.cooksd,cov.ratios,large.residual) %>%
dplyr::filter(large.residual == TRUE)
# showing rows with Cook's distance > 1
housing_data.lev %>%
filter(.cooksd > 1)
housing_data.lev <- housing_data_set1.augment %>%
dplyr::select(Sale.Price, bedrooms,bath_full_count,.hat,.cooksd,cov.ratios,large.residual) %>%
dplyr::filter(large.residual == TRUE)
# showing rows with Cook's distance > 1
housing_data.lev %>%
filter(.cooksd > 1)
# showing rows with Cook's distance > 1
housing_data.lev %>%
filter(.hat > .0006)
housing_data.lev <- housing_data_set1.augment %>%
dplyr::select(Sale.Price, bedrooms,bath_full_count,.hat,.cooksd,cov.ratios,large.residual) %>%
dplyr::filter(large.residual == TRUE)
# showing rows with Cook's distance > 1
housing_data.lev %>%
filter(.cooksd > 1)
# showing rows with leverage > .0006
housing_data.lev %>%
filter(.hat > .0006)
# showing rows with cov ratio > 1.0006 & < .9994
housing_data.lev %>%
filter(cov.ratios > 1.0006 | cov.ratios < .9994)
dwt(var2)
install.packages("durbinWatsonTest")
dwtest(var2)
install.packages("lmtest")
library(lmtest)
dwtest(var2)
vif(var2)
install.packages("VIF")
library(vif)
library(VIF)
vif(var2)
library(VIF)
library(VIF)
vif(var2)
head(housing_data_set1.augment)
head(housing_data_set1.augment)
ggplot(housing_data_set1.augment, aes(x = .fitted, y = Sale.Price)) + geom_point()
knitr::opts_chunk$set(echo = FALSE)
head(housing_data_set1.augment)
# ggplot(housing_data_set1.augment, aes(x = .fitted, y = Sale.Price)) + geom_point()
head(housing_data_set1.augment)
histogram<-ggplot(housing_data_set1.augment, aes(.std.resid)) + opts(legend.position =
"none") + geom_histogram(aes(y = ..density..), colour = "black", fill = "white") +
labs(x = "Standardized Residual", y = "Density")
head(housing_data_set1.augment)
histogram <- ggplot(housing_data_set1.augment, aes(.std.resid)) +
opts(legend.position = "none") +
geom_histogram(aes(y = ..density..), colour = "black", fill = "white") +
labs(x = "Standardized Residual", y = "Density")
head(housing_data_set1.augment)
histogram <- ggplot(housing_data_set1.augment, aes(.std.resid)) +
# opts(legend.position = "none") +
geom_histogram(aes(y = ..density..), colour = "black", fill = "white") +
labs(x = "Standardized Residual", y = "Density")
histogram +
stat_function(fun = dnorm, args = list(mean = mean(housing_data_set1.augment$.std.resid, na.rm = TRUE),
sd = sd(housing_data_set1.augment$.std.resid, na.rm = TRUE)), colour = "red", size = 1)
# head(housing_data_set1.augment)
scatter <- ggplot(housing_data_set1.augment, aes(x = .fitted, y= .std.resid))
scatter + geom_point() +
geom_smooth(method = "lm", colour = "Blue") +
labs(x = "FittedValues", y = "Standardized Residual")
knitr::opts_chunk$set(echo = FALSE)
ggplot(data = housing_data_set1, aes(x = bedrooms, y = `Sale Price`, color = as.factor(bath_full_count))) +
geom_point() +
labs( title = "Plotting Sale Price against Lot size",
x = "No. Of Bedrooms",
y = "Sale Price")
knitr::opts_chunk$set(echo = FALSE)
library(dplyr)
library(ggplot2)
library(corrplot)
library(readxl)
#populating the housing_data dataframe
wd <- getwd()
fname <- "week-7-housing.xlsx"
path_to_file <- paste(wd,'/dataset/',fname, sep = "")
path_to_file
housing_data <- read_excel(path_to_file,col_names=TRUE)
glimpse(housing_data)
head(housing_data)
#Selecting only some variables from the dataframe into our analysis dataframe set 1
housing_data_set1 <- housing_data %>% dplyr::select(`Sale Price`,bedrooms,bath_full_count,sq_ft_lot)
head(housing_data_set1)
ggplot(data = housing_data_set1, aes(x = sq_ft_lot, y = `Sale Price`)) +
geom_point() +
labs( title = "Plotting Sale Price against Lot size",
x = "Lot size in sq ft",
y = "Sale Price")
var1 <- lm(`Sale Price` ~ sq_ft_lot, data = housing_data_set1)
ggplot(data = housing_data_set1, aes(x = bedrooms, y = `Sale Price`, color = as.factor(bath_full_count))) +
geom_point() +
labs( title = "Plotting Sale Price against Lot size",
x = "No. Of Bedrooms",
y = "Sale Price")
ggplot(data = housing_data_set1, aes(x = bedrooms, y = `Sale Price`, color = as.factor(bath_full_count))) +
geom_point() + geom_jitter() +
labs( title = "Plotting Sale Price against # of bedrooms",
x = "No. Of Bedrooms",
y = "Sale Price")
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(echo = FALSE)
library(ggplot2)
library(dplyr)
#populating the housing_data dataframe
wd <- getwd()
fname <- "binary-classifier-data.csv"
path_to_file <- paste(wd,'/dataset/',fname, sep = "")
path_to_file
my_init_df <- read.csv(path_to_file, header = TRUE)
summary(my_init_df)
head(my_init_df)
# Creating a new data frame from the initial dataframe
binary_data <- my_init_df
my_plot <- ggplot(data = binary_data, aes(x = x, y = y, col = as.factor(label)))
my_plot <- my_plot + geom_point()
my_plot <- my_plot + xlab("X") + ylab("Y") + scale_color_discrete(name = "Label")
my_plot
#random selection of 70% data.
set.seed(123)
dat.d <- sample(1:nrow(binary_data),size=nrow(binary_data)*0.7,replace = FALSE)
train.binary_data <- binary_data[dat.d,] # 70% training data
test.binary_data <- binary_data[-dat.d,] # remaining 30% test data
dim(train.binary_data)
dim(test.binary_data)
train.binary_data.glm <- glm(label ~ x + y, data = train.binary_data, family = binomial)
summary(train.binary_data.glm)
pred <- predict(train.binary_data.glm, newdata = test.binary_data, type = "response")
# Confusion matrix
table_matrix <- table(test.binary_data$label, pred)
table_matrix
pred <- predict(train.binary_data.glm, newdata = test.binary_data, type = "response")
# Confusion matrix
table_matrix <- table(test.binary_data$label, pred > 0.5)
table_matrix
View(test.binary_data)
accuracy_test_glm <- sum(diag(table_matrix)) / sum(table_matrix)
accuracy_test_glm
# Grouping the new data frame into 2 clusters: km_binary_data
km_binary_data <- kmeans(km_binary_data.df, 2)
# Grouping the new data frame into 2 clusters: km_binary_data
km_binary_data <- kmeans(binary_data.df, 2)
# Grouping the new data frame into 2 clusters: km_binary_data
km_binary_data <- kmeans(binary_data, 2)
#visualizing the cluster using plot
plot(km_binary_data, col = km_binary_data$cluster)
# Grouping the new data frame into 2 clusters: km_binary_data
km_binary_data <- kmeans(binary_data, 2)
#visualizing the cluster using plot
plot(y ~ x, data = binary_data, col = km_binary_data$cluster)
km_binary_data$centers
ggplot(data = binary_data,aes(y = y, x = x,col = km_binary_data$cluster)) + geom_point()
ggplot(data = binary_data,aes(y = y, x = x,col = as.factor(km_binary_data$cluster))) + geom_point()
# Grouping the new data frame into 2 clusters: km_binary_data
km_binary_data <- kmeans(binary_data, 2)
#visualizing the cluster using plot
ggplot(data = binary_data,aes(y = y, x = x,col = as.factor(km_binary_data$cluster))) + geom_point() + xlab("X") + ylab("Y") + scale_color_discrete(name = "Label")
km_binary_data$centers
knitr::opts_chunk$set(echo = FALSE)
library(foreign)
library(ggplot2)
library(dplyr)
#populating the housing_data dataframe
wd <- getwd()
fname <- "ThoraricSurgery.arff"
path_to_file <- paste(wd,'/dataset/',fname, sep = "")
path_to_file
my_init_df <- read.arff(path_to_file)
glimpse(my_init_df)
summary(my_init_df)
head(my_init_df)
# Building the dataframe for analysis
survive <- my_init_df
survive$Risk1Yr <- ifelse(survive$Risk1Yr=="T",1,0)
survive$Risk1Yr <- as.factor(survive$Risk1Yr)
summary(survive)
# load library
library(broom)
# Applying the glm function for the logistic regression
survive_m <- glm(Risk1Yr ~., data = survive, family = binomial(link = "logit"))
summary(survive_m)
# Using tidy function from the broom package
out_survive_m <- tidy(survive_m)
# Looking for the snapshot of the output of the model
## out_survive_m
#Ordering the output based on the p value
out_survive_m[order(out_survive_m$estimate, decreasing = TRUE),]
# augment(survive_m)
pred <- predict(survive_m, newdata = survive, type ="response")
table_matrix <- table(survive$Risk1Yr, pred > 0.5)
table_matrix
# augment(survive_m)
pred <- predict(survive_m, newdata = survive, type ="response")
table_matrix <- table(survive$Risk1Yr, pred > 0.5)
table_matrix
accuracy_test_glm <- sum(diag(table_matrix)) / sum(table_matrix)
accuracy_test_glm
# Before we start the K Nearest Neighbor algorithm we need to remove the outcome variable from the Train and test data sets
# Storing the label column from the Train and Test Set as Train_label and Test_label
train_label <- train.binary_data$label
test_label <- test.binary_data$label
# Copy the Train and Test Dataset as knn_train and knn_test
knn_train <- train.binary_data
knn_test <- test.binary_data
# Drop the label column from the knn data sets
knn_train$label <- NULL
knn_test$label <- NULL
install.packages("class")
library(class)
# Before we start the K Nearest Neighbor algorithm we need to remove the outcome variable from the Train and test data sets
# Storing the label column from the Train and Test Set as Train_label and Test_label
train_label <- train.binary_data$label
test_label <- test.binary_data$label
# Copy the Train and Test Dataset as knn_train and knn_test
knn_train <- train.binary_data
knn_test <- test.binary_data
# Drop the label column from the knn data sets
knn_train$label <- NULL
knn_test$label <- NULL
# Now use knn() from the class package
library(class)
# use knn() to predict the values of the test set based on 5 neighbors.
# It needs 4 arguments cl: factor of true class labels of the training set
set.seed(1)
pred <- knn(train = knn_train, test = knn_test, cl = train_label, k = 5)
# Construct the Confusion matrix making Test_labels as rows
conf <- table(test_label, pred)
conf
set.seed(1)
# Create range, a vector of K values to try and accs vector to store the accuracies for these different values
range <- 1:round(0.2 * nrow(knn_train))
accs <- rep(0, length(range))
for (k in range) {
pred <- knn(train = knn_train, test = knn_test, cl = train_label, k = k)
conf <- table(test_label, pred)
accs[k] <- sum(diag(conf)) / sum(conf)
}
set.seed(1)
# Create range, a vector of K values to try and accs vector to store the accuracies for these different values
range <- 1:round(0.2 * nrow(knn_train))
accs <- rep(0, length(range))
for (k in range) {
pred <- knn(train = knn_train, test = knn_test, cl = train_label, k = k)
conf <- table(test_label, pred)
# Derive the accuracy and assign it to the correct index in accs
accs[k] <- sum(diag(conf)) / sum(conf)
}
# Plot the accuracies. Title of x-axis is "k".
ggplot(data = pred, aes(x = range, y = accs)) + geom_point()
set.seed(1)
# Create range, a vector of K values to try and accs vector to store the accuracies for these different values
range <- 1:round(0.2 * nrow(knn_train))
accs <- rep(0, length(range))
df <- for (k in range) {
pred <- knn(train = knn_train, test = knn_test, cl = train_label, k = k)
conf <- table(test_label, pred)
# Derive the accuracy and assign it to the correct index in accs
accs[k] <- sum(diag(conf)) / sum(conf)
}
# Plot the accuracies. Title of x-axis is "k".
ggplot(data = df, aes(x = range, y = accs)) + geom_point()
ggplot(data = df, aes(x = range, y = accs)) + geom_jitter()
ggplot(data = df, aes(x = range, y = accs)) + geom_point() + geom_jitter(height = 0.5)
ggplot(data = df, aes(x = range, y = accs)) + geom_point() + geom_jitter(width = 0.5)
which.max(accs)
# Before we start the K Nearest Neighbor algorithm we need to remove the outcome variable from the Train and test data sets
# Storing the label column from the Train and Test Set as Train_label and Test_label
train_label <- train.binary_data$label
test_label <- test.binary_data$label
# Copy the Train and Test Dataset as knn_train and knn_test
knn_train <- train.binary_data
knn_test <- test.binary_data
# Drop the label column from the knn data sets
knn_train$label <- NULL
knn_test$label <- NULL
# Now use knn() from the class package
library(class)
# use knn() to predict the values of the test set based on 5 neighbors.
# It needs 4 arguments cl: factor of true class labels of the training set
set.seed(1)
pred <- knn(train = knn_train, test = knn_test, cl = train_label, k = 10)
# Construct the Confusion matrix making Test_labels as rows
conf <- table(test_label, pred)
conf
set.seed(1)
# Create range, a vector of K values to try and accs vector to store the accuracies for these different values
range <- 1:round(0.2 * nrow(knn_train))
accs <- rep(0, length(range))
df <- for (k in range) {
pred <- knn(train = knn_train, test = knn_test, cl = train_label, k = k)
conf <- table(test_label, pred)
# Derive the accuracy and assign it to the correct index in accs
accs[k] <- sum(diag(conf)) / sum(conf)
}
# Plot the accuracies. Title of x-axis is "k".
ggplot(data = df, aes(x = range, y = accs)) + geom_point()
which.max(accs)
# Before we start the K Nearest Neighbor algorithm we need to remove the outcome variable from the Train and test data sets
# Storing the label column from the Train and Test Set as Train_label and Test_label
train_label <- train.binary_data$label
test_label <- test.binary_data$label
# Copy the Train and Test Dataset as knn_train and knn_test
knn_train <- train.binary_data
knn_test <- test.binary_data
# Drop the label column from the knn data sets
knn_train$label <- NULL
knn_test$label <- NULL
# Now use knn() from the class package
library(class)
# use knn() to predict the values of the test set based on 5 neighbors.
# It needs 4 arguments cl: factor of true class labels of the training set
set.seed(1)
pred <- knn(train = knn_train, test = knn_test, cl = train_label, k = 10)
# Construct the Confusion matrix making Test_labels as rows
conf <- table(test_label, pred)
conf
accuracy_knn <- sum(diag(conf)) / sum(conf)
# Before we start the K Nearest Neighbor algorithm we need to remove the outcome variable from the Train and test data sets
# Storing the label column from the Train and Test Set as Train_label and Test_label
train_label <- train.binary_data$label
test_label <- test.binary_data$label
# Copy the Train and Test Dataset as knn_train and knn_test
knn_train <- train.binary_data
knn_test <- test.binary_data
# Drop the label column from the knn data sets
knn_train$label <- NULL
knn_test$label <- NULL
# Now use knn() from the class package
library(class)
# use knn() to predict the values of the test set based on 5 neighbors.
# It needs 4 arguments cl: factor of true class labels of the training set
set.seed(1)
pred <- knn(train = knn_train, test = knn_test, cl = train_label, k = 10)
# Construct the Confusion matrix making Test_labels as rows
conf <- table(test_label, pred)
conf
accuracy_knn <- sum(diag(conf)) / sum(conf)
accuracy_knn
# Before we start the K Nearest Neighbor algorithm we need to remove the outcome variable from the Train and test data sets
# Storing the label column from the Train and Test Set as Train_label and Test_label
train_label <- train.binary_data$label
test_label <- test.binary_data$label
# Copy the Train and Test Dataset as knn_train and knn_test
knn_train <- train.binary_data
knn_test <- test.binary_data
# Drop the label column from the knn data sets
knn_train$label <- NULL
knn_test$label <- NULL
# Now use knn() from the class package
library(class)
# use knn() to predict the values of the test set based on 5 neighbors.
# It needs 4 arguments cl: factor of true class labels of the training set
set.seed(1)
pred <- knn(train = knn_train, test = knn_test, cl = train_label, k = 5)
# Construct the Confusion matrix making Test_labels as rows
conf <- table(test_label, pred)
conf
accuracy_knn <- sum(diag(conf)) / sum(conf)
accuracy_knn
set.seed(1)
# Create range, a vector of K values to try and accs vector to store the accuracies for these different values
range <- 1:round(0.2 * nrow(knn_train))
accs <- rep(0, length(range))
for (k in range) {
pred <- knn(train = knn_train, test = knn_test, cl = train_label, k = k)
conf <- table(test_label, pred)
# Derive the accuracy and assign it to the correct index in accs
accs[k] <- sum(diag(conf)) / sum(conf)
}
# Plot the accuracies. Title of x-axis is "k".
# ggplot(data = df, aes(x = range, y = accs)) + geom_point()
plot(range, accs, xlab = "k")
which.max(accs)
set.seed(1)
# Create range, a vector of K values to try and accs vector to store the accuracies for these different values
range <- 1:round(0.2 * nrow(knn_train))
accs <- rep(0, length(range))
df <- for (k in range) {
pred <- knn(train = knn_train, test = knn_test, cl = train_label, k = k)
conf <- table(test_label, pred)
# Derive the accuracy and assign it to the correct index in accs
accs[k] <- sum(diag(conf)) / sum(conf)
}
# Plot the accuracies. Title of x-axis is "k".
ggplot(data = df, aes(x = range, y = accs)) + geom_point()
## plot(range, accs, xlab = "k")
which.max(accs)
library(ROCR)
install.packages("ROCR")
library(ROCR)
install.packages("ROCR")
library(ROCR)
install.packages("ROCR")
install.packages("caTools")
install.packages("gplots")
install.packages("ROCR")
install.packages("ROCR")
reticulate::repl_python()
import datetime as dt
import pandas as pd
import pandas_datareader.data as web
import matplotlib.pyplot as plt
from matplotlib import style
yes
