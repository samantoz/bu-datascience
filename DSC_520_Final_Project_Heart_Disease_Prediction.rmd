---
Name: "Arindam Samanta"
Date: February 27 2020
Title: "Prediction of Heart Disease based on Health Data"
output:
  
  word_document: default
---

```{r setup,echo=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```



# Section 1: Getting Started
## Introduction
World Health Organization has estimated 12 million deaths occur worldwide, every year due to Heart diseases. Half the deaths in the United States and other developed countries are due to cardio vascular diseases. The early prognosis of cardiovascular diseases can aid in making decisions on lifestyle changes in high risk patients and in turn reduce the complications. This analysis intends to pinpoint the most relevant/risk factors of heart disease as well as predict the overall risk using logistic regression and random forest algorithms.

## Research question
We would like to do Exploratory Data Analysis on the data to get an understanding of the different features and how they are related to one another. We would also look at the variables and see if they are related to one another.As part of this exercise we are trying to build a classification problem to answer our main question about the chances of having a heart disease based on the various medical prognosis and tests.

1) What are the chances of having a heart disease based on the Vital data collected from a sample of patients of different age group and occupation?
2) What is the most significant predictor for a Heart disease?
3) Is their a significant correlation between the feature variables that could predict chances of having a heart disease for a particular individual?
4) Is there a possibility that based on the analysis of the vital data if we could prevent a possible heart failure from happening?
5) How far can it be predicted that a heart disease is possible based on the cholesterol, blood pressure

## Approach
I am planning to use Logistic Regression algorithm using glm and random forest in order to do a comparison to the output. The random forest algorithm works by aggregating the predictions made by multiple decision trees. I would be using bootstrapped dataset created from the original dataset.

## How my approach addresses (fully or partially) the problem
When the random forest is used for classification and is presented with a new sample, the final prediction is made by taking the majority of the predictions made by each individual decision tree in the forest. In the event, it is used for regression and it is presented with a new sample, the final prediction is made by taking the average of the predictions made by each individual decision tree in the forest.

## Data
I have looked into various sources for this dataset that could be helpful in predicting the possible heart failure for an individual based on patient data that are collected for each of the patients on a regular checkup.
Finally the below dataset was found to be most suitable. It was available in kaggle also.

Link to the dataset: https://archive.ics.uci.edu/ml/datasets/Heart+Disease

This dataset integrates all the databases present in Heart Disease Dataset available at UCI Machine Learning Repository. Original one contains 4 databases: Cleveland, Hungarian, Long Beach, and Switzerland. Most of the work has been done using Cleveland dataset only.
   The authors of the databases have requested:

      ...that any publications resulting from the use of the data include the 
      names of the principal investigator responsible for the data collection
      at each institution.  They would be:

       1. Hungarian Institute of Cardiology. Budapest: Andras Janosi, M.D.
       2. University Hospital, Zurich, Switzerland: William Steinbrunn, M.D.
       3. University Hospital, Basel, Switzerland: Matthias Pfisterer, M.D.
       4. V.A. Medical Center, Long Beach and Cleveland Clinic Foundation:
	  Robert Detrano, M.D., Ph.D.

## Required Packages

I am planning to use the following packages to start with and some more might be needed as I start the work:
package : tidyr
package : dplyr
package : ggplot2
package : broom
package : randomForest
package : caTools

## Plots and Table

Tables:
Showing the first few rwos of the data set

Plot:
scatter plot showing the distribution of the important features
3-d plot if needed for the data
histogram


## Questions for future steps

As a starting point I would like to answer the following questions:
1) What is the distribution of the data for the important features?
2) How does the features have there values normalized or do they have biases?
3) Is there any collinearity between the variables/features?
4) What are most imporatnt features from the list of all the features available in the dataset?

# Section 2 (Week 10)

Loading the required libraries.

```{r , echo=TRUE, results='hide', message=FALSE, warning=FALSE}
library(tidyr)
library(dplyr)
library(ggplot2)
library(broom)
library(randomForest)

```
# Q1: Data importing and cleaning steps are explained in the text and in the DataCamp exercises.(Tell me why you are doing the data cleaning activities that you perform). Follow a logical process.

## Explaining the steps on how to import and clean my data
### A brief overview about the source data
The actual database contains 76 attributes, but all published experiments refer to using a subset of 14 of them.  In particular, the Cleveland database is the only one that has been used by ML researchers to this date. But we would be using all the other datasets also. The "goal" field refers to the presence of heart disease in the patient.  It is integer valued from 0 (no presence) to 4. Experiments with the Cleveland database have concentrated on simply attempting to distinguish presence (values 1,2,3,4) from absence (value 0).  
   
The names and social security numbers of the patients were removed from the database, replaced with dummy values. We are using all four processed files which also exist in the dataset directory.

### Summary of the various datasets are given below. 
        Database:    # of instances:
          Cleveland: 303
          Hungarian: 294
        Switzerland: 123
      Long Beach VA: 200

### Attribute Information: The attributes that are defined in the below datasets are defined here. 

It also shows the position of the attributes in the actual files:
   Only 14 used:
       V1. #3  (age)       : Age in years    
       V2. #4  (sex)       : sex (1 = male; 0 = female)
       V3. #9  (cp)        : chest pain type (1:typical angina, 2:atypical angina,
                                                3:non-anginal pain, 4: asymptomatic)
       V4. #10 (trestbps)  : resting blood pressure (in mm Hg on admission to the hospital)
       V5. #12 (chol)      : serum cholestoral in mg/dl
       V6. #16 (fbs)       : (fasting blood sugar > 120 mg/dl)  (1 = true; 0 = false)
       V7. #19 (restecg)   : resting electrocardiographic results 
                                0: normal, 
                                1: having ST-T wave abnormality (T wave inversions and/or ST 
                                                      elevation or depression of > 0.05 mV)
                                2: showing probable or definite left ventricular hypertrophy
                                                      by Estes' criteria
       V8. #32 (thalach)   : maximum heart rate achieved
       V9. #38 (exang)     : exercise induced angina (1 = yes; 0 = no)
       V10. #40 (oldpeak)  : ST depression induced by exercise relative to rest  
       V11. #41 (slope)    : the slope of the peak exercise ST segment
                                1: upsloping
                                2: flat
                                3: downsloping
       V12. #44 (ca)       : number of major vessels (0-3) colored by flourosopy 
       V13. #51 (thal)     : 3 = normal; 6 = fixed defect; 7 = reversable defect  
       V14. #58 (num)      : the predicted attribute
                                diagnosis of heart disease (angiographic disease status)
                                 0: < 50% diameter narrowing (No heart disease)
                                 1: > 50% diameter narrowing ( Yes Heart disease)

### Explanation for why clean up is necessary

Before we apply any algorithm on our data, it is obvious that the data should be tidy or structured. But in the real world, the data mostly we initially see is unstructured. So in order to make it tidy and to further apply any algorithm to derive the insights, data has to be cleaned. The major reason why the data is not tidy is because of the presence of missing values and outliers. So as a first step I would start by importing each of the 4 files which have already being processed, into 4 different data frames and looking at those separately in order to get a better understanding of the values of the attributes and how they are distributed. 

### Given below is the detailed step-by-step process of cleaning the data. 

```{r , echo=TRUE, results='asis', message=FALSE, warning=FALSE}
#populating the cleveland file into dataframe
wd <- getwd()
cleveland_f <- "processed.cleveland.data"
path_to_file <- paste(wd,'/dataset/',cleveland_f, sep = "")
## path_to_file

cleveland_df <- read.csv(path_to_file, header = FALSE)
## dim(cleveland_df)
## summary(cleveland_df)
## head(cleveland_df)

#populating the Hungarian file into dataframe
hungarian_f <- "processed.hungarian.data"
path_to_file <- paste(wd,'/dataset/',hungarian_f, sep = "")
## path_to_file

hungarian_df <- read.csv(path_to_file, header = FALSE)
## dim(hungarian_df)
## summary(hungarian_df)
## head(hungarian_df)

#populating the Switzerland file into dataframe
switzerland_f <- "processed.switzerland.data"
path_to_file <- paste(wd,'/dataset/',switzerland_f, sep = "")
## path_to_file

switzerland_df <- read.csv(path_to_file, header = FALSE)
## dim(switzerland_df)
## summary(switzerland_df)
## head(switzerland_df)

#populating the Long Beach, CA data file into dataframe
long_beach_f <- "processed.va.data"
path_to_file <- paste(wd,'/dataset/',long_beach_f, sep = "")
## path_to_file

long_beach_df <- read.csv(path_to_file, header = FALSE)
## dim(long_beach_df)
## summary(long_beach_df)
## head(long_beach_df)

dim(cleveland_df)
dim(hungarian_df)
dim(switzerland_df)
dim(long_beach_df)

```
After having a initial look at the datasets, now adding the 4 individual dataframes into separate data datasets and also adding the column names for each dataset in order to do some more analysis on each of those, as we know that the data do not have the column names.

```{r , echo=TRUE, results='hide', message=FALSE, warning=FALSE}
# Creating new data frames with the existing dataframe towards our final dataset.
data.cle <- cleveland_df
data.hun <- hungarian_df
data.sw <- switzerland_df
data.va <- long_beach_df

```

### Adding the new columns for each of the data source name. 
Also adding an extra variable to keep the Source name along with the data set. Now looking at the data briefly. This would help in the model building phase after I combine all the datasets.
```{r , echo=TRUE, results='hide', message=FALSE, warning=FALSE}
names(data.cle) <- c("age", "sex", "cp", "trestbps", "chol", "fbs", "restecg", "thalach", "exang", "oldpeak", "slope", "ca", "thal", "out")
names(data.hun) <- c("age", "sex", "cp", "trestbps", "chol", "fbs", "restecg", "thalach", "exang", "oldpeak", "slope", "ca", "thal", "out")
names(data.sw) <- c("age", "sex", "cp", "trestbps", "chol", "fbs", "restecg", "thalach", "exang", "oldpeak", "slope", "ca", "thal", "out")
names(data.va) <- c("age", "sex", "cp", "trestbps", "chol", "fbs", "restecg", "thalach", "exang", "oldpeak", "slope", "ca", "thal", "out")

# Adding the extra column to identify the source for the file
data1 <- data.cle %>% mutate(datasrc = "Cleveland") 
data2 <- data.hun %>% mutate(datasrc = "Hungarian") 
data3 <- data.sw %>% mutate(datasrc = "Switzerland") 
data4 <- data.va %>% mutate(datasrc = "Long Beach VA")

head(data1)
head(data2)
head(data3)
head(data4)

```

We are trying to clean up the dataset individually.

### Working on cleaning and tidying the Cleveland Dataset
Here for our problem, we are only going to attempt to distinguish the presence of heart disease (values 1,2,3,4) from absence of heart disease (value 0). Therefore, we replace all labels greater than 1 by 1. 
Then taking summary of each of the data sets. Showing below our findings on each of the various datasets.

```{r , echo=TRUE, results='hide', message=FALSE, warning=FALSE}

data1$out[data1$out > 0] <- 1
summary(data1)
glimpse(data1)

```
We see that the data set is showing mean for categorical variables also. Hence we need to re-specify the column types. We know a categorical variable (a variable that takes on a finite amount of values) is a factor. As we can see, sex is incorrectly treated as a number when in reality it can only be 1 if male and 0 if female. We can use the transform method to change the in built type of each feature.
```{r , echo=TRUE, results='markup', message=FALSE, warning=FALSE}

data1 <- transform(
  data1,
  age=as.integer(age),
  sex=as.factor(sex),
  cp=as.factor(cp),
  trestbps=as.integer(trestbps),
  chol=as.integer(chol),
  fbs=as.factor(fbs),
  restecg=as.factor(restecg),
  thalach=as.integer(thalach),
  exang=as.factor(exang),
  oldpeak=as.numeric(oldpeak),
  slope=as.factor(slope),
  ca=as.factor(ca),
  thal=as.factor(thal),
  out=as.factor(out),
  datasrc=as.character(datasrc)
)
summary(data1)
glimpse(data1)
```
Now also we find that there are missing values for some of the variables in each dataset. Here we see missing/unknown values for the below variables.
ca       : number of major vessels (0-3) colored by flourosopy 
thal     : 3 = normal; 6 = fixed defect; 7 = reversable defect  
Now after replacing the missing values ("?") with NA we can use the colSums function to count the number of missing values. It shows that we have 4 and 2 rows with missing values for thal and ca respectively. 

```{r , echo=TRUE, results='asis', message=FALSE, warning=FALSE}
data1[ data1 == "?"] <- NA
colSums(is.na(data1))

```
Let us have a look at those 6 rows and see if we could find anything interesting in those rows.

```{r , echo=TRUE, results='asis', message=FALSE, warning=FALSE}
data1 %>% filter(is.na(ca) | is.na(thal))
```
Now as we are not experts in this type of data let us take these 6 rows out from the cleveland dataset.Now looking at the new cleaner dataset we see there are no missing records now and the count of the dataset is 303 - 6 = 296.
This is only being done on the Cleveland dataset as that seems to be missing the least number of variables. We would use this dataset set separately as our final data set for prediction purposes.

```{r , echo=TRUE, results='markup', message=FALSE, warning=FALSE}
data11 <- data1 %>% filter(!is.na(ca)) %>%  filter(!is.na(thal))
colSums(is.na(data11))

# Recasting the above 2 columns as factor shows the summary info correctly
data11$ca <- factor(data11$ca)
data11$thal <- factor(data11$thal)
summary(data11)
```
### Repeating the same above processes on the remaining dataset and coming up with a clean/tidy dataset

```{r , echo=TRUE, results='hide', message=FALSE, warning=FALSE}
## Hungarian Dataset
data2$out[data2$out > 1] <- 1
## summary(data2)
## glimpse(data2)

data2 <- transform(
  data2,
  age=as.integer(age),
  sex=as.factor(sex),
  cp=as.factor(cp),
  trestbps=as.integer(trestbps),
  chol=as.integer(chol),
  fbs=as.factor(fbs),
  restecg=as.factor(restecg),
  thalach=as.integer(thalach),
  exang=as.factor(exang),
  oldpeak=as.numeric(oldpeak),
  slope=as.factor(slope),
  ca=as.factor(ca),
  thal=as.factor(thal),
  out=as.factor(out),
  datasrc=as.character(datasrc)
)
## summary(data2)
## glimpse(data2)

data2[ data2 == "?"] <- NA
colSums(is.na(data2))
```

```{r , echo=TRUE, results='hide', message=FALSE, warning=FALSE}
## Swiss Dataset
data3$out[data3$out > 1] <- 1
## summary(data2)
## glimpse(data2)

data3 <- transform(
  data3,
  age=as.integer(age),
  sex=as.factor(sex),
  cp=as.factor(cp),
  trestbps=as.integer(trestbps),
  chol=as.integer(chol),
  fbs=as.factor(fbs),
  restecg=as.factor(restecg),
  thalach=as.integer(thalach),
  exang=as.factor(exang),
  oldpeak=as.numeric(oldpeak),
  slope=as.factor(slope),
  ca=as.factor(ca),
  thal=as.factor(thal),
  out=as.factor(out),
  datasrc=as.character(datasrc)
)
## summary(data2)
## glimpse(data2)

data3[ data3 == "?"] <- NA
colSums(is.na(data3))
```

```{r , echo=TRUE, results='hide', message=FALSE, warning=FALSE}
## Long Beach, VA Dataset
data4$out[data4$out > 1] <- 1
## summary(data2)
## glimpse(data2)

data4 <- transform(
  data4,
  age=as.integer(age),
  sex=as.factor(sex),
  cp=as.factor(cp),
  trestbps=as.integer(trestbps),
  chol=as.integer(chol),
  fbs=as.factor(fbs),
  restecg=as.factor(restecg),
  thalach=as.integer(thalach),
  exang=as.factor(exang),
  oldpeak=as.numeric(oldpeak),
  slope=as.factor(slope),
  ca=as.factor(ca),
  thal=as.factor(thal),
  out=as.factor(out),
  datasrc=as.character(datasrc)
)
## summary(data2)
## glimpse(data2)

data4[ data4 == "?"] <- NA
colSums(is.na(data4))
```


### Combining the dataframes together now. 
This is a combined data set with all the 4 datasets combined into one big dataset.But after looking at the various data sets it became clear that except for the Cleveland dataset all others do have a lot of missing data for some of the variables. 
Showing the summary on the combined dataset it looks like that the dataset from Long Beach VA and Swiss has a significantly higher number of outcomes that shows more patients having heart disease(out = 1) than without (out = 0). 
```{r , echo=TRUE, results='hide', message=FALSE, warning=FALSE}
combined.data <- rbind(data1,data2,data3,data4)

combined.data %>% group_by(datasrc,out) %>% summarise(n())
## glimpse(combined.data)
## head(combined.data)
colSums(is.na(combined.data))
```
## What does the final data set look like
With a clean dataset, show what the final data set looks like. However, do not print off a data frame with 200+ rows; show me the data in the most condensed form possible.
So after doing the initial round of Exploratory Data Analysis, I am planning on using the Clean and Tidy Cleveland data set for my predictions. I will name it the final_data. The summary detail is shown below. This dataset do not have any missing values or outliers.
```{r , echo=TRUE, results='markup', message=FALSE, warning=FALSE}
final_data <- data11

glimpse(final_data)
colSums(is.na(final_data))
summary(final_data)
```
## Questions for future step
    What do you not know how to do right now that you need to learn to answer your questions?
    I do not know confidently how to manage datasets having missing values for some of the variables. I am planning on learning that for my next steps. So as of now I am planning on using the Cleveland data set as that is mostly complete and so would help in the model building.
    
    Do you plan on incorporating any machine learning techniques to answer your research questions? Explain.
    I am planning on applying random forest model for this classification problem along with the Generalized Linear Model and see the effectiveness of each models prediction. 
    Also before I apply any machine learning techniques I am planning on visualizing the dataset using scatter plots for some of the variables.(e.g, age,sex and some others) 
    Also I would like to find the correlation and test between the various predictor variables.

# Step: 3
# Date: 02/23/2020

## Discuss how you plan to uncover new information in the data that is not self-evident.

Taking a summary of the final dataset I see that age(int), trestbps - resting blood pressure (on admission), chol - serum cholestorol are all measured against different scales and should be compared with each other.

```{r , echo=TRUE, results='markup', message=FALSE, warning=FALSE}

glimpse(final_data)
summary(final_data)

```

Trying to see if there is a high correlation among the independant variables that could lead to multicollinearity problem
For this we would use the library called psych. Also we are trying to find the correlation among the numeric variables that are independant to each other. In the below diagram, we see the scatter plot showing the correlation coefficient among the independant variables. It shows that there are no significant correlation between the variables and hence they are truly independant. The highest correlation is 0.29 between age and resting blood pressure and the next is between age and serum cholestorol is 0.20. But none of these are high. (>.80).
Hence this relationship is not self evident from the data which I found out using the tool.
So no multicollinerity problem exists here.

```{r , echo=TRUE, results='markup', message=FALSE, warning=FALSE}
library(psych)

mydata.cor <- final_data %>% select(age,trestbps,chol,thalach,sex)

head(mydata.cor)

cor(mydata.cor[,-5])

# Not including the binary column Sex for correlation test
pairs.panels(mydata.cor[,-5],
             gap = 0,
             bg = c("red","blue")[mydata.cor$sex],
             pch = 21)

```
### Looking for multicollinerity among the following variables


```{r , echo=TRUE, results='markup', message=FALSE, warning=FALSE}
library(psych)

## glimpse(final_data)

mydata.cor1 <- final_data %>% select(age,chol,ca,sex)

head(mydata.cor1)

## cor(mydata.cor1[,-5])

# Not including the binary column Sex for correlation test
pairs.panels(mydata.cor1[,-4],
             gap = 0,
             bg = c("red","blue")[mydata.cor1$sex],
             pch = 21)

```
### Plotting the data to see any story

```{r , echo=TRUE, results='markup', message=FALSE, warning=FALSE}
## glimpse(final_data)

mydata.plot1 <- final_data %>% select(age,chol,ca,sex)

head(mydata.plot1)

ggplot(data = mydata.plot1, aes(x = age, y = chol, color = ca)) + 
  geom_point() + 
  facet_wrap(~sex, nrow = 2)

```

## What are different ways you could look at this data to answer the questions you want to answer?

Using the final dataset that I considered in my last analysis is the Cleveland dataset as it has almost all the variables with complete values. The assumption of the existence of heart disease here is since we're trying to predict significant heart disease those with higher level stenoses(>50%) would be treated as heart disease. I am not able to find any other values as the dataset has been made with the 50% diameter narrowing as the cut off point. 
First I am trying to plot histograms to show how the age and sex are related to the presence of the heart disease for the combined dataset. As per my earlier data analysis age and sex variables did not have any missing values in the combined dataset. It shows that Long Beach, VA and Swiss datasets females have less representation like 6 and 10 only. Also we can see that 


```{r , echo=TRUE, results='markup', message=FALSE, warning=FALSE}

combined.data %>% group_by(datasrc,sex) %>% summarise(n())


p <- ggplot(data = combined.data, aes(age)) 
p <- p + geom_histogram(aes(fill = sex))
p <-  p+ scale_fill_discrete(name="Sex",
                         labels=c("Female", "Male"))
p <- p + labs(title="Histogram for Age") + labs(x="Age", y="Count")
p + facet_wrap(~datasrc)

```
## Do you plan to slice and dice the data in different ways, create new variables, or join separate data frames to create new summary information? Explain.
## How could you summarize your data to answer key questions?


Now looking at the summary of the data by Output, it shows that surprisingly both Swiss and Long beach datasets have more people with Heart disease than not. It could be that only the patients who had severe problem did only come to do the checkup. So my decision of picking up the Cleveland datset for my model training was a good choice as that would be less biased.

```{r , echo=TRUE, results='markup', message=FALSE, warning=FALSE}

combined.data %>% group_by(datasrc,out) %>% summarise(n())

p <- ggplot(combined.data, aes(x = age, y = out)) + geom_point() + geom_jitter()
p + facet_wrap(~datasrc)

```

## What types of plots and tables will help you to illustrate the findings to your questions? Ensure that all graph plots have axis titles, legend if necessary, scales are appropriate, appropriate geoms used, etc.).

I have used the above correlation plots to see if there are any significant collinearity between the various numeric predictors.

## What do you not know how to do right now that you need to learn to answer your questions?
I am trying to understand if all the predictors are significant for the output variable. I am planning to do some more tests to get that clarified.

## Do you plan on incorporating any machine learning techniques to answer your research questions? Explain.

My approach here is to apply logistic regression and random forest model, and wish to analyse the likelihood of a patient having heart disease based on changes to the above independent variables. 
Note that in this case, we are not looking to obtain a specific coefficient value but rather an odds ratio; a ratio that tells us the likelihood of an occurance of heart disease based on any independent variable.

Let us start building the model. We would split the data first into Training and Test Set with a 80, 20 split.

```{r , echo=TRUE, results='markup', message=FALSE, warning=FALSE}
# Loading package caret for data splitting
library(caret)
library(e1071)
set.seed(1234)

# Creating the data partition with 80 20 split between training and test sets
in_train <- createDataPartition(final_data$out, p = 0.8, list = FALSE)

training <- final_data[in_train,]
test <- final_data[-in_train,]

# Looking at the training set
glimpse(training)

#Training a random Forest Model

fit_rf <- train(out~., method = "rf", data = training[-15], trcontrol = trainControl(method = "none"))

# Building a confusion Matrix on the random forest model

cm_rf <- confusionMatrix(predict(fit_rf, test[-15]), test$out)

# Training a Generalised Logistic model with binomial distribution

fit_glm <- train(out~., method = "glm",family = "binomial", data = training[-15])

# Building the confusion matrix on the logistic regression model
cm_glm <- confusionMatrix(predict(fit_glm, test[-15]), test$out)

# Summary and the confusion matrix on  the random forest model
summary(fit_rf)
cm_rf

# Summary and the confusion matrix on  the logistic regression model
summary(fit_glm)
cm_glm



```
Looking at the confusion matrix it looks like the predictions that I am getting from both the models are on the high accuracy (> 80%).
But I am planning to look into more details into the outputs and comparing the different values of Sensitivity and specificity for both the models.

 ### Points to be noted for Final submission..
 
 #### Describe the categorical variables: The variables are self-explanatory and refer to the attributes of cars and the response variable is ‘Car Acceptability’. All the variables are categorical in nature and have 3-4 factor levels in each.
 
 ##### Find the correlation between, age,cholesterol, weight and Gender also.
 
