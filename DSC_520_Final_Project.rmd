---
Name: "Arindam Samanta"
Date: February 02 2020
Title: "<TBD>"
output:
  
  word_document: default
---

```{r setup,echo=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```
Example : https://towardsdatascience.com/random-forest-in-r-f66adf80ec9


# Section 1: Getting Started
## Introduction
World Health Organization has estimated 12 million deaths occur worldwide, every year due to Heart diseases. Half the deaths in the United States and other developed countries are due to cardio vascular diseases. The early prognosis of cardiovascular diseases can aid in making decisions on lifestyle changes in high risk patients and in turn reduce the complications. This analysis intends to pinpoint the most relevant/risk factors of heart disease as well as predict the overall risk using logistic regression and random forest algorithms.

## Research question
We would like to do Exploratory Data Analysis on the data to get an understanding of the different features and how they are related to one another. We would also look at the variables and see if they are related to one another.We are trying to build a classification problem to answer our main question about the chances of having a heart disease.

1) What are the chances of having a heart disease based on the Vital data collected from a patient?
2) What is the most significant predictor for a Heart disease?
3) Is their a correlation between Age and Sex with the chances of having a heart disease?
4) Is there a possibility that based on the analysis of the vital data if we could prevent a possible heart failure from happening?
5) 

## Approach
I am planning to use Logistic Regression algorithm using glm and random forest in order to do a comparison to the output. The random forest algorithm works by aggregating the predictions made by multiple decision trees. I would be using bootstrapped dataset created from the original dataset.

## How my approach addresses (fully or partially) the problem
When the random forest is used for classification and is presented with a new sample, the final prediction is made by taking the majority of the predictions made by each individual decision tree in the forest. In the event, it is used for regression and it is presented with a new sample, the final prediction is made by taking the average of the predictions made by each individual decision tree in the forest.

## Data
I have looked into various sources for this dataset that could be helpful in predicting the possible heart failure for an individual based on patient data that are collected for each of the patients on a regular checkup.
Finally the below dataset was found to be most suitable. It was available in kaggle also.

Link to the dataset: https://archive.ics.uci.edu/ml/datasets/Heart+Disease

This dataset integrates all the databases present in Heart Disease Dataset available at UCI Machine Learning Repository. Original one contains 4 databases: Cleveland, Hungarian, Long Beach, and Switzerland. Most of the work has been done using Cleveland dataset only.
   The authors of the databases have requested:

      ...that any publications resulting from the use of the data include the 
      names of the principal investigator responsible for the data collection
      at each institution.  They would be:

       1. Hungarian Institute of Cardiology. Budapest: Andras Janosi, M.D.
       2. University Hospital, Zurich, Switzerland: William Steinbrunn, M.D.
       3. University Hospital, Basel, Switzerland: Matthias Pfisterer, M.D.
       4. V.A. Medical Center, Long Beach and Cleveland Clinic Foundation:
	  Robert Detrano, M.D., Ph.D.

## Required Packages

I am planning to use the following packages to start with and some more might be needed as I start the work:
package : tidyr
package : dplyr
package : ggplot2
package : broom
package : randomForest
package : caTools

## Plots and Table

Tables:
Showing the first few rwos of the data set

Plot:
scatter plot showing the distribution of the important features
3-d plot if needed for the data
histogram


## Questions for future steps

As a starting point I would like to answer the following questions:
1) What is the distribution of the data for the important features?
2) How does the features have there values normalized or do they have biases?
3) Is there any collinearity between the variables/features?
4) What are most imporatnt features from the list of all the features available in the dataset?

# Section 2 (Week 10)

Loading the required libraries.

```{r , echo=TRUE, results='hide', message=FALSE, warning=FALSE}
library(tidyr)
library(dplyr)
library(ggplot2)
library(broom)
library(randomForest)

```
## How to import and clean my data
### About the source data
The actual database contains 76 attributes, but all published experiments refer to using a subset of 14 of them.  In particular, the Cleveland database is the only one that has been used by ML researchers to this date. But we would be using all the other datasets also. The "goal" field refers to the presence of heart disease in the patient.  It is integer valued from 0 (no presence) to 4. Experiments with the Cleveland database have concentrated on simply attempting to distinguish presence (values 1,2,3,4) from absence (value 0).  
   
The names and social security numbers of the patients were removed from the database, replaced with dummy values. We are using all four processed files which also exist in the dataset directory.

### Summary of the various datasets are given below. 
        Database:    # of instances:
          Cleveland: 303
          Hungarian: 294
        Switzerland: 123
      Long Beach VA: 200

### Attribute Information: The attributes that are defined in the below datasets are defined here. It also shows the position of the attributes in the actual files.
   -- Only 14 used
      -- V1. #3  (age)       : Age in years    
      -- V2. #4  (sex)       : sex (1 = male; 0 = female)
      -- V3. #9  (cp)        : chest pain type (1:typical angina, 2:atypical angina,
                                                3:non-anginal pain, 4: asymptomatic)
      -- V4. #10 (trestbps)  : resting blood pressure (in mm Hg on admission to the hospital)
      -- V5. #12 (chol)      : serum cholestoral in mg/dl
      -- V6. #16 (fbs)       : (fasting blood sugar > 120 mg/dl)  (1 = true; 0 = false)
      -- V7. #19 (restecg)   : resting electrocardiographic results 
                                0: normal, 
                                1: having ST-T wave abnormality (T wave inversions and/or ST 
                                                      elevation or depression of > 0.05 mV)
                                2: showing probable or definite left ventricular hypertrophy
                                                      by Estes' criteria
      -- V8. #32 (thalach)   : maximum heart rate achieved
      -- V9. #38 (exang)     : exercise induced angina (1 = yes; 0 = no)
      -- V10. #40 (oldpeak)  : ST depression induced by exercise relative to rest  
      -- V11. #41 (slope)    : the slope of the peak exercise ST segment
                                1: upsloping
                                2: flat
                                3: downsloping
      -- V12. #44 (ca)       : number of major vessels (0-3) colored by flourosopy 
      -- V13. #51 (thal)     : 3 = normal; 6 = fixed defect; 7 = reversable defect  
      -- V14. #58 (num)      : the predicted attribute
                                diagnosis of heart disease (angiographic disease status)
                                 0: < 50% diameter narrowing (No heart disease)
                                 1: > 50% diameter narrowing ( Yes Heart disease)

I would start by importing each of the 4 files which have already being processed, into 4 different data frames and looking at those separately in order to get a better understanding of the values of the attributes and how they are distributed. 

```{r , echo=TRUE, results='hide', message=FALSE, warning=FALSE}
#populating the cleveland file into dataframe
wd <- getwd()
cleveland_f <- "processed.cleveland.data"
path_to_file <- paste(wd,'/dataset/',cleveland_f, sep = "")
path_to_file

cleveland_df <- read.csv(path_to_file, header = FALSE)
## dim(cleveland_df)
## summary(cleveland_df)
## head(cleveland_df)

#populating the Hungarian file into dataframe
hungarian_f <- "processed.hungarian.data"
path_to_file <- paste(wd,'/dataset/',hungarian_f, sep = "")
path_to_file

hungarian_df <- read.csv(path_to_file, header = FALSE)
## dim(hungarian_df)
## summary(hungarian_df)
## head(hungarian_df)

#populating the Switzerland file into dataframe
switzerland_f <- "processed.switzerland.data"
path_to_file <- paste(wd,'/dataset/',switzerland_f, sep = "")
path_to_file

switzerland_df <- read.csv(path_to_file, header = FALSE)
## dim(switzerland_df)
## summary(switzerland_df)
## head(switzerland_df)

#populating the Long Beach, CA data file into dataframe
long_beach_f <- "processed.va.data"
path_to_file <- paste(wd,'/dataset/',long_beach_f, sep = "")
path_to_file

long_beach_df <- read.csv(path_to_file, header = FALSE)
## dim(long_beach_df)
## summary(long_beach_df)
## head(long_beach_df)

dim(cleveland_df)
dim(hungarian_df)
dim(switzerland_df)
dim(long_beach_df)

```
After having a initial look at the datasets, now adding the 4 individual dataframes into separate data datasets and also adding the column names for each dataset in order to do some more analysis on each of those, as we know that the data do not have the column names.

```{r , echo=TRUE, results='hide', message=FALSE, warning=FALSE}
# Creating new data frames with the existing dataframe towards our final dataset.
data.cle <- cleveland_df
data.hun <- hungarian_df
data.sw <- switzerland_df
data.va <- long_beach_df

```

### Adding the new columns for each of the data source name. 
Also adding an extra variable to keep the Source name along with the data set. Now looking at the data briefly.
```{r , echo=TRUE, results='hide', message=FALSE, warning=FALSE}
names(data.cle) <- c("age", "sex", "cp", "trestbps", "chol", "fbs", "restecg", "thalach", "exang", "oldpeak", "slope", "ca", "thal", "out")
names(data.hun) <- c("age", "sex", "cp", "trestbps", "chol", "fbs", "restecg", "thalach", "exang", "oldpeak", "slope", "ca", "thal", "out")
names(data.sw) <- c("age", "sex", "cp", "trestbps", "chol", "fbs", "restecg", "thalach", "exang", "oldpeak", "slope", "ca", "thal", "out")
names(data.va) <- c("age", "sex", "cp", "trestbps", "chol", "fbs", "restecg", "thalach", "exang", "oldpeak", "slope", "ca", "thal", "out")

# Adding the extra column to identify the source for the file
data1 <- data.cle %>% mutate(datasrc = "Cleveland") 
data2 <- data.hun %>% mutate(datasrc = "Hungarian") 
data3 <- data.sw %>% mutate(datasrc = "Switzerland") 
data4 <- data.va %>% mutate(datasrc = "Long Beach VA")

head(data1)
head(data2)
head(data3)
head(data4)

```

We are trying to clean up the dataset individually.

### Working on cleaning and tidying the Cleveland Dataset
Here for our problem, we are only going to attempt to distinguish the presence of heart disease (values 1,2,3,4) from absence of heart disease (value 0). Therefore, we replace all labels greater than 1 by 1. 
Then taking summary of each of the data sets. Showing below our findings on each of the various datasets.

```{r , echo=TRUE, results='hide', message=FALSE, warning=FALSE}

data1$out[data1$out > 1] <- 1
summary(data1)
glimpse(data1)

```
We see that the data set is showing mean for categorical variables also. Hence we need to re-specify the column types. We know a categorical variable (a variable that takes on a finite amount of values) is a factor. As we can see, sex is incorrectly treated as a number when in reality it can only be 1 if male and 0 if female. We can use the transform method to change the in built type of each feature.
```{r , echo=TRUE, results='hide', message=FALSE, warning=FALSE}

data1 <- transform(
  data1,
  age=as.integer(age),
  sex=as.factor(sex),
  cp=as.factor(cp),
  trestbps=as.integer(trestbps),
  chol=as.integer(chol),
  fbs=as.factor(fbs),
  restecg=as.factor(restecg),
  thalach=as.integer(thalach),
  exang=as.factor(exang),
  oldpeak=as.numeric(oldpeak),
  slope=as.factor(slope),
  ca=as.factor(ca),
  thal=as.factor(thal),
  out=as.factor(out),
  datasrc=as.character(datasrc)
)
summary(data1)
glimpse(data1)
```
Now also we find that there are missing values for some of the variables in each dataset. Here we see missing/unknown values for the below variables.
ca       : number of major vessels (0-3) colored by flourosopy 
thal     : 3 = normal; 6 = fixed defect; 7 = reversable defect  
Now after replacing the missing values ("?") with NA we can use the colSums function to count the number of missing values. It shows that we have 4 and 2 rows with missing values for thal and ca respectively. 

```{r , echo=TRUE, results='hide', message=FALSE, warning=FALSE}
data1[ data1 == "?"] <- NA
colSums(is.na(data1))

```
Let us have a look at those rows and see if we could find anything interesting in those rows.

```{r , echo=TRUE, results='hide', message=FALSE, warning=FALSE}
data1 %>% filter(is.na(ca) | is.na(thal))
```
Now as we are not experts in this type of data let us take these 6 rows out from the cleveland dataset.Now looking at the new cleaner dataset we see there are no missing records now and the count of the dataset is 303 - 6 = 296.
```{r , echo=TRUE, results='hide', message=FALSE, warning=FALSE}
data11 <- data1 %>% filter(!is.na(ca)) %>% filter(!is.na(thal))
colSums(is.na(data11))

summary(data11)
```
### Repeating the same above processes on the remaining dataset and coming up with a clean/tidy dataset

```{r , echo=TRUE, results='hide', message=FALSE, warning=FALSE}
## Hungarian Dataset
data2$out[data2$out > 1] <- 1
## summary(data2)
## glimpse(data2)

data2 <- transform(
  data2,
  age=as.integer(age),
  sex=as.factor(sex),
  cp=as.factor(cp),
  trestbps=as.integer(trestbps),
  chol=as.integer(chol),
  fbs=as.factor(fbs),
  restecg=as.factor(restecg),
  thalach=as.integer(thalach),
  exang=as.factor(exang),
  oldpeak=as.numeric(oldpeak),
  slope=as.factor(slope),
  ca=as.factor(ca),
  thal=as.factor(thal),
  out=as.factor(out),
  datasrc=as.character(datasrc)
)
## summary(data2)
## glimpse(data2)

data2[ data2 == "?"] <- NA
colSums(is.na(data2))
```

```{r , echo=TRUE, results='hide', message=FALSE, warning=FALSE}
## Swiss Dataset
data3$out[data3$out > 1] <- 1
## summary(data2)
## glimpse(data2)

data3 <- transform(
  data3,
  age=as.integer(age),
  sex=as.factor(sex),
  cp=as.factor(cp),
  trestbps=as.integer(trestbps),
  chol=as.integer(chol),
  fbs=as.factor(fbs),
  restecg=as.factor(restecg),
  thalach=as.integer(thalach),
  exang=as.factor(exang),
  oldpeak=as.numeric(oldpeak),
  slope=as.factor(slope),
  ca=as.factor(ca),
  thal=as.factor(thal),
  out=as.factor(out),
  datasrc=as.character(datasrc)
)
## summary(data2)
## glimpse(data2)

data3[ data3 == "?"] <- NA
colSums(is.na(data3))
```

```{r , echo=TRUE, results='hide', message=FALSE, warning=FALSE}
## Long Beach, VA Dataset
data4$out[data4$out > 1] <- 1
## summary(data2)
## glimpse(data2)

data4 <- transform(
  data4,
  age=as.integer(age),
  sex=as.factor(sex),
  cp=as.factor(cp),
  trestbps=as.integer(trestbps),
  chol=as.integer(chol),
  fbs=as.factor(fbs),
  restecg=as.factor(restecg),
  thalach=as.integer(thalach),
  exang=as.factor(exang),
  oldpeak=as.numeric(oldpeak),
  slope=as.factor(slope),
  ca=as.factor(ca),
  thal=as.factor(thal),
  out=as.factor(out),
  datasrc=as.character(datasrc)
)
## summary(data2)
## glimpse(data2)

data4[ data4 == "?"] <- NA
colSums(is.na(data4))
```


data2$out[data2$out > 1] <- 1
summary(data2)

data3$out[data3$out > 1] <- 1
summary(data3)

data4$out[data4$out > 1] <- 1
summary(data4)

```

data1 %>% group_by(out) %>% summarise(n())

# stacking the dataframes together now
combined.data <- rbind(data1,data2,data3,data4)

combined.data %>% group_by(datasrc,out) %>% summarise(n())

head(combined.data)
```
## What does the final data set look like
With a clean dataset, show what the final data set looks like. However, do not print off a data frame with 200+ rows; show me the data in the most condensed form possible.
    What do you not know how to do right now that you need to learn to import and cleanup your dataset?
    Discuss how you plan to uncover new information in the data that is not self-ent to answer?
    Do you plan to slice and dice the data in different ways, create new variables, or join separate data frames to create new summary information? Explain.
    How could you summarize your data to answer key questions?vident.
    What are different ways you could look at this data to answer the questions you wa
    What types of plots and tables will help you to illustrate the findings to your questions? Ensure that all graph plots have axis titles, legend if necessary, scales are appropriate, appropriate geoms used, etc.).
    


## Questions for future step
    What do you not know how to do right now that you need to learn to answer your questions?
    Do you plan on incorporating any machine learning techniques to answer your research questions? Explain.
